"""
This scripts creates a nerfStudio-compatible dataset from a room/office of the Replica
dataset. Note that the files in the replica room/office come from two origins. The two
sequences were generated by the authors of semantic-NeRF using habitat-sim. The other
files were downloaded using Replica github.

Dropbox with semantic-NeRF generated sequences:
https://www.dropbox.com/sh/9yu1elddll00sdl/AABWLTQhDhTQ7vCS8PZmrSmJa/Replica_Dataset?dl=0&subfolder_nav_tracking=1

Replica github to download the rest of the files:
https://github.com/facebookresearch/Replica-Dataset
"""

import json
import shutil
from dataclasses import dataclass
from pathlib import Path
from typing import Any, List
import cv2

import numpy as np
import trimesh
import tyro
from imgviz import label_colormap
from open3d import io, visualization
from PIL import Image
from plyfile import PlyData, PlyElement
import re

modulo = 8


@dataclass
class PathsToDatasets:
    """Stores input arguments used for creating the nerf dataset."""

    path_to_replica_room: Path = Path("./")
    """Path to the replica datastet (one room/office)"""
    output_path: Path = Path("/home/antoine-laborde/Documents/datasets/")
    """Path to the folder where to store the NeRF dataset"""

    def __post_init__(self) -> None:
        dataset_name = self.path_to_replica_room.name + "_for_NeRF_mini"
        self.output_path = self.output_path / dataset_name
        self.output_path.mkdir(parents=True, exist_ok=True)

        self.path_to_replica_room = self.path_to_replica_room / "Sequence_1"
        if not self.path_to_replica_room.is_dir():
            raise FileExistsError("Replica dataset doesn't contain a camera sequence.")


def get_semantic_point_cloud(
    point_cloud_rgb_and_index: PlyData,
    id_to_label: list[int],
    label_to_colour: np.ndarray,
) -> tuple[PlyElement, list[tuple[int, int, int]]]:
    """
    From a point cloud `point_cloud_rgb_and_index` that associates vertices with a RGB
    color and faces (defined by 4 vertices) with an index (intance index), we want a
    point cloud that associates each vertex to a color. The algorithm is inspired from
    the Delaunay triangulation. `id_to_label` maps this instance index to a semantic
    labels and `label_to_colour` maps this label to a semantic RGB colour.

    :returns: vertices associated to a semantic RGB colour and the list of used semantic
    RGB colors
    """
    vertices = point_cloud_rgb_and_index.elements[0]
    faces = point_cloud_rgb_and_index.elements[1]

    vertices_output = []
    semantic_colour_list = []
    for face in faces:
        vertex_positions_tuple = vertices[face["vertex_indices"]][["x", "y", "z"]]
        vertex_positions_array = np.array(
            [list(position) for position in vertex_positions_tuple]
        )

        middle_position = np.mean(vertex_positions_array, axis=0).tolist()
        semantic_colour = label_to_colour[id_to_label[face["object_id"]]].tolist()
        middle_position.extend(semantic_colour)
        vertices_output.append(tuple(middle_position))

        if tuple(semantic_colour) not in semantic_colour_list:
            semantic_colour_list.append(tuple(semantic_colour))

    vertices_output = np.array(
        vertices_output,
        dtype=[
            ("x", "f4"),
            ("y", "f4"),
            ("z", "f4"),
            ("red", "u1"),
            ("green", "u1"),
            ("blue", "u1"),
        ],
    )

    return PlyElement.describe(vertices_output, "vertex"), semantic_colour_list


def opencv_to_opengl_camera(transform: np.ndarray) -> np.ndarray:
    return transform @ trimesh.transformations.rotation_matrix(
        np.deg2rad(180), [1, 0, 0]
    )


def get_cameras_transform(paths: PathsToDatasets) -> List[np.ndarray]:
    cameras_to_world = []
    with open(paths.path_to_replica_room / "traj_w_c.txt", "r") as f:
        line = f.readline().split()
        while len(line) != 0:
            cam_to_world = np.zeros((4, 4))
            for i, number in enumerate(line):
                cam_to_world[i // 4][i % 4] = float(number)
            cameras_to_world.append(cam_to_world)
            line = f.readline().split()

    return cameras_to_world


def get_meta_data(paths: PathsToDatasets, width: int, height: int) -> dict[str, Any]:
    """Creates a dict filled with informations needed by the semantic-SDF dataparser"""
    global modulo

    meta_data = {
        "camera_model": "OPEN_CV",
        "width": width,
        "height": height,
        "has_mono_prior": True,
        "has_foreground_mask": False,
        "has_sparse_sfm_points": False,
        "scene_box": {"aabb": [[-1, -1, -1], [1, 1, 1]]},
        "frames": [],
    }
    cameras_to_world = get_cameras_transform(paths)
    for image_number, cam_to_world in enumerate(cameras_to_world):
        if image_number % modulo == 0:
            cam_to_world = opencv_to_opengl_camera(cam_to_world)

            fx = 0.5 * width / np.tan((90 / 2.0) * np.pi / 180)
            intrinsic = [
                [fx, 0, width / 2, 0],
                [0, fx, height / 2, 0],
                [0, 0, 1, 0],
                [0, 0, 0, 1],
            ]

            new_view = {
                "rgb_path": "rgb_{}.png".format(image_number),
                "segmentation_path": "segmentation_{}.png".format(image_number),
                "depth_path": "depth_{}.png".format(image_number),
                "normals_path": "normals_{}.png".format(image_number),
                "camtoworld": cam_to_world.tolist(),
                "intrinsics": intrinsic,
            }
            meta_data["frames"].append(new_view)

    return meta_data


def get_normal_map(depth: np.ndarray) -> np.ndarray:
    height, width = depth.shape
    fx = 0.5 * width * np.tan((90 * np.pi / 180) / 2.0)
    K = [
        [fx, 0, width / 2],
        [0, fx, height / 2],
        [0, 0, 1],
    ]

    def normalization(data):
        mo_chang = np.sqrt(
            np.multiply(data[:, :, 0], data[:, :, 0])
            + np.multiply(data[:, :, 1], data[:, :, 1])
            + np.multiply(data[:, :, 2], data[:, :, 2])
        )
        mo_chang = np.dstack((mo_chang, mo_chang, mo_chang))
        return data / mo_chang

    x, y = np.meshgrid(np.arange(0, width), np.arange(0, height))
    x = x.reshape([-1])
    y = y.reshape([-1])
    xyz = np.vstack((x, y, np.ones_like(x)))
    pts_3d = np.dot(np.linalg.inv(K), xyz * depth.reshape([-1]))
    pts_3d_world = pts_3d.reshape((3, height, width))
    f = (
        pts_3d_world[:, 1 : height - 1, 2:width]
        - pts_3d_world[:, 1 : height - 1, 1 : width - 1]
    )
    t = (
        pts_3d_world[:, 2:height, 1 : width - 1]
        - pts_3d_world[:, 1 : height - 1, 1 : width - 1]
    )
    normal_map = np.cross(f, t, axisa=0, axisb=0)
    normal_map = normalization(normal_map)

    # Padding to keep initial size
    normals = np.zeros((height, width, 3))
    normals[1:-1, 1:-1] = normal_map
    normals[1:-1, 0] = normal_map[:, 0]
    normals[1:-1:, -1] = normal_map[:, -1]
    normals[0, :] = normals[1, :]
    normals[-1, :] = normals[-2, :]

    return normals


def main() -> None:
    global modulo
    paths = tyro.cli(PathsToDatasets)

    # Get RGB images
    got_image_size = False
    for source_rgb in (paths.path_to_replica_room / "rgb").iterdir():
        image_number = int(re.findall(r"\d+", source_rgb.name)[0])
        if image_number % modulo == 0:
            if not got_image_size:
                got_image_size = True
                image = np.asarray(Image.open(source_rgb))
                height = image.shape[0]
                width = image.shape[1]

            shutil.copyfile(source_rgb, paths.output_path / source_rgb.name)

    # Get segmented images
    for i, source_semantic in enumerate(
        (paths.path_to_replica_room / "semantic_class").iterdir()
    ):
        image_number = int(re.findall(r"\d+", source_semantic.name)[0])
        if image_number % modulo == 0:
            new_name = "segmentation" + source_semantic.name[13:]
            shutil.copyfile(source_semantic, paths.output_path / new_name)

    # Get depth images and normals
    cameras_to_world = get_cameras_transform(paths=paths)
    for source_depth in (paths.path_to_replica_room / "depth").iterdir():
        image_number = int(re.findall(r"\d+", source_depth.name)[0])
        cam_to_world = cameras_to_world[image_number]
        if image_number % modulo == 0:
            depth = cv2.imread(str(source_depth), flags=cv2.IMREAD_ANYDEPTH)
            normals = get_normal_map(depth)

            normals = normals.reshape((height * width, 3))
            normals = cam_to_world[:3, :3] @ normals.T
            normals = (normals.T).reshape((height, width, 3))

            normals = -normals
            normals = np.uint8((normals + 1) / 2 * 255)[..., ::-1]
            shutil.copyfile(source_depth, paths.output_path / source_depth.name)
            cv2.imwrite(
                str(paths.output_path / "normals_{}.png".format(image_number)), normals
            )

    # Get meta_data.json with camera parameters and transform matrixes
    meta_data = get_meta_data(paths, width, height)

    with open(paths.output_path / "meta_data.json", "w") as outfile:
        json.dump(meta_data, outfile, indent=4)

    # Get semantic point cloud ground truth
    with open(
        paths.path_to_replica_room / "../habitat/info_semantic.json", "r"
    ) as semantic_info:
        info = json.load(semantic_info)
        id_to_label = np.array(info["id_to_label"])
        id_to_label[id_to_label <= 0] = 0

    point_cloud_rgb_and_index = PlyData.read(
        paths.path_to_replica_room / "../habitat/mesh_semantic.ply"
    )

    vertices, semantic_colour_list = get_semantic_point_cloud(
        point_cloud_rgb_and_index, id_to_label, label_colormap()
    )

    # Save semantic point cloud
    PlyData([vertices]).write(paths.output_path / "semantic_mesh.ply")

    # Get segmentation metadata
    dict_label_colour_map = {
        str(i): list(color) for i, color in enumerate(semantic_colour_list)
    }
    with open(paths.output_path / "segmentation.json", "w") as outfile:
        json.dump(dict_label_colour_map, outfile, indent=4)

    # Visualize semantic point cloud for verification
    cloud = io.read_point_cloud(str(paths.output_path / "semantic_mesh.ply"))
    visualization.draw_geometries([cloud])  # type: ignore


if __name__ == "__main__":
    main()
